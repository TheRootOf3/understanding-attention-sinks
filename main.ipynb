{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L46 Mini Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Optional, Tuple, Literal\n",
    "import json\n",
    "import math\n",
    "import types\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaAttention,\n",
    "    rotate_half,\n",
    "    repeat_kv,\n",
    ")\n",
    "\n",
    "from transformers.cache_utils import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change appropriately\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen models\n",
    "\n",
    "GPT2 and Llama3.2-1B.\n",
    "\n",
    "TODO: \n",
    "- [ ] Write about them and their specifics. (PEs, attention, dimensions, num parameters)\n",
    "- [ ] Show example generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_tokenizer(\n",
    "    model_path: str,\n",
    "    output_attn: bool = False,\n",
    "    attn_implementation: Optional[str] = None,\n",
    ") -> tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
    "    # TODO add docstring\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_path,\n",
    "        output_attentions=output_attn,\n",
    "        attn_implementation=attn_implementation,\n",
    "    ).to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_model_path = \"/Users/aszab/repos/models/Llama-3.2-1B\"\n",
    "llama3_model, llama3_tokenizer = load_model_tokenizer(\n",
    "    llama3_model_path, attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model_path = \"/Users/aszab/repos/models/gpt2\"\n",
    "gpt2_model, gpt2_tokenizer = load_model_tokenizer(\n",
    "    gpt2_model_path, attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    num_tokens: int = 10,\n",
    ") -> list[str]:\n",
    "    # TODO add docstring\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generations = []\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "            )\n",
    "        pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        inputs[\"input_ids\"] = torch.cat([inputs[\"input_ids\"], pred_token_idx], dim=-1)\n",
    "        inputs[\"attention_mask\"] = torch.cat(\n",
    "            [inputs[\"attention_mask\"], torch.ones_like(pred_token_idx)], dim=-1\n",
    "        )\n",
    "        pred_token = tokenizer.decode(pred_token_idx[0])\n",
    "        generations.append(pred_token)\n",
    "\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 example generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I love him. I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm\n",
      "\n",
      "---\n",
      "1.89 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prompt = \"Hello, my dog is cute and\"\n",
    "print(\"\".join(generate_text(prompt, gpt2_model, gpt2_tokenizer, 100)))\n",
    "print(f\"\\n---\\n{time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLama3.2-1B generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " playful, but he is very aggressive and he bites me when I try to pet him. I have tried to train him, but he is very stubborn and he doesn’t listen to me. I have tried to use a leash, but he still bites\n",
      "\n",
      "---\n",
      "8.06 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prompt = \"Hello, my dog is cute and\"\n",
    "print(\"\".join(generate_text(prompt, llama3_model, llama3_tokenizer, 100)))\n",
    "print(f\"\\n---\\n{time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Explain why attention scales quadratically. (Maybe show one of the graphs from the website? https://medium.com/@joaolages/kv-caching-explained-276520203249)\n",
    "\n",
    "Instead of calculating the attention components at each decoding step, we can reuse the already computed keys and values by caching them and using in the next decoding step.\n",
    "\n",
    "Conveniently, the default implementation of both GPT2 and Llama3.2 from the `transformers` library returns the computed keys and values during a generation in the `outputs.past_key_values` object of type `tuple[tuple[torch.Tensor[batch_size, attn_head_num, seq_num, attn_head_dim]]]`, where the first tuple is of the dimension of the number of decoder layers, while the second is 2-dimensional with the first element being keys and the latter being values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_kv_caching(\n",
    "    prompt: str,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    num_tokens: int = 10,\n",
    "    kv_cache_size: int = -1,\n",
    ") -> list[str]:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generations = []\n",
    "\n",
    "    # First compute all the KV caches for the prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "        )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    generations.append(\n",
    "        tokenizer.decode(outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)[0])\n",
    "    )\n",
    "\n",
    "    # Now generate the rest of the tokens one by one\n",
    "    for _ in range(num_tokens):\n",
    "        pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                pred_token_idx,\n",
    "                labels=None,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        if kv_cache_size != -1 and past_key_values[0][0].shape[2] >= kv_cache_size:\n",
    "            past_key_values = [\n",
    "                [param[:, :, -kv_cache_size:, :] for param in layer]\n",
    "                for layer in past_key_values\n",
    "            ]\n",
    "\n",
    "        generations.append(\n",
    "            tokenizer.decode(outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)[0])\n",
    "        )\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "prompt = \"Hello, my dog is cute and\"\n",
    "print(\n",
    "    \"\".join(\n",
    "        generate_text_kv_caching(\n",
    "            prompt,\n",
    "            gpt2_model,\n",
    "            gpt2_tokenizer,\n",
    "            100,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(f\"\\n---\\n{time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " playful, but he is very aggressive and he bites me when I try to pet him. I have tried to train him, but he is very stubborn and he doesn’t listen to me. I have tried to use a leash, but he still bites me\n",
      "\n",
      "---\n",
      "2.59 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prompt = \"Hello, my dog is cute and\"\n",
    "print(\n",
    "    \"\".join(\n",
    "        generate_text_kv_caching(\n",
    "            prompt,\n",
    "            llama3_model,\n",
    "            llama3_tokenizer,\n",
    "            100,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(f\"\\n---\\n{time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cache capacity is exceeded, some of the tokens need to be evicted. The default cache implementation, also known as *window attention* uses the least recently used (LRU) cache eviction policy. Let's set the `kv_cache_size`=20 tokens and observe the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " playful, but he is very aggressive and he bites me when I try, he dog bites me bites me aggressive aggressive aggressive aggressive aggressive aggressive aggressive bites bites bites bites bites bites bites biteseseseseseseseseseseseseseseses\n",
      "\n",
      "---\n",
      "2.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prompt = \"Hello, my dog is cute and\"\n",
    "print(\n",
    "    \"\".join(\n",
    "        generate_text_kv_caching(\n",
    "            prompt,\n",
    "            llama3_model,\n",
    "            llama3_tokenizer,\n",
    "            50,\n",
    "            kv_cache_size=20,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(f\"\\n---\\n{time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the original paper by Xiao et al., the model's ability to generate text drastically vanishes. The authors argue that it is caused by ejecting the position from the kv cache, which in fact serves as an attention sink -- a place to \"dump\" attention score resulting from its softmax formulation. \n",
    "\n",
    "#TODO Describe more why this happens\n",
    "\n",
    "They propose a novel method, called sink attention, which modifies the cache eviction policy by always keeping the first *n* tokens.\n",
    "\n",
    "#TODO add attention diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "The first experiment serves as a way to replicate and deepen the results obtained by Xiao et al., using a new generation model -- Llama3.2-1B. \n",
    "\n",
    "Following Xiao et al., we implement 4 types of attention computations:\n",
    "- Dense attention\n",
    "- Window attention\n",
    "- Sliding window with recomputation\n",
    "- Sink attention\n",
    "\n",
    "We compare the 4 implementation using a commonly used test split of the `wikitext-2` dataset to evaluate model perplexity, generation time and kv-cache memory usage of GPT-2 and LLama3.2.\n",
    "\n",
    "### Implementation of sink attention forward pass \n",
    "\n",
    "However, implementing sink attention additionally involves shifting the positional encodings, which are added to the keys and values for attention computation. This allows the model to decode sequences even past the maximum sequence length on which it was trained.\n",
    "\n",
    "Most modern models use Rotary Positional Embeddings (RoPE), relative positional encodings or ALiBi (Attention with Linear Biases), which directly affect how attention is computed. Llama3.2 uses RoPEs, which are added to the keys and queries before they are stored in the kv cache. Since sink attention involves shifting positions of stored keys and values, it is necessary to reimplement the attention pass for the model. To do so, I will modify the [original `transformers` implementation of the *eager attention* forward pass](https://github.com/huggingface/transformers/blob/241c04d36867259cdf11dbb4e9d9a60f9cb65ebc/src/transformers/models/llama/modeling_llama.py#L290-L358) and replace all its calls with the modified version.\n",
    "\n",
    "#TODO explain that the positional encodings need to be recomputed each time the attention pass is performed but some computation can be saved if implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb_x(x, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
    "    return x_embed\n",
    "\n",
    "\n",
    "def llama_modified_attention_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "    query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    # Figure out the length of the key value sequence given current and cached key value states\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    if past_key_value is not None and len(past_key_value) > self.layer_idx:\n",
    "        kv_seq_len += past_key_value[self.layer_idx][0].shape[-2]\n",
    "\n",
    "    # If we have a cache, we can use it to avoid recomputing the key value states.\n",
    "    # NOTE: Keys are stored without positional encodings added.\n",
    "    if past_key_value is not None:\n",
    "        key_states, value_states = past_key_value.update(\n",
    "            key_states,\n",
    "            value_states,\n",
    "            self.layer_idx,\n",
    "        )\n",
    "\n",
    "    # Recompute rotary_emb for the entire sequence. In theory this could be done once when the model is instantiated.\n",
    "    cos, sin = self.rotary_emb(\n",
    "        value_states,\n",
    "        position_ids=torch.arange(kv_seq_len, device=value_states.device).unsqueeze(0),\n",
    "    )\n",
    "\n",
    "    # Compute keys and queries with rotary embeddings separately given they may have different lengths due to caching.\n",
    "    key_states = apply_rotary_pos_emb_x(key_states, cos, sin)\n",
    "    query_states = apply_rotary_pos_emb_x(\n",
    "        query_states, cos[:, -q_len:, :], sin[:, -q_len:, :]\n",
    "    )\n",
    "\n",
    "    # Continue with the original transformers Llama implementation\n",
    "    key_states = repeat_kv(\n",
    "        key_states,\n",
    "        self.num_key_value_groups,\n",
    "    )\n",
    "    value_states = repeat_kv(\n",
    "        value_states,\n",
    "        self.num_key_value_groups,\n",
    "    )\n",
    "    attn_weights = torch.matmul(\n",
    "        query_states,\n",
    "        key_states.transpose(2, 3),\n",
    "    ) / math.sqrt(self.head_dim)\n",
    "\n",
    "    if attention_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    attn_weights = nn.functional.softmax(\n",
    "        attn_weights,\n",
    "        dim=-1,\n",
    "        dtype=torch.float32,\n",
    "    ).to(query_states.dtype)\n",
    "    attn_weights = nn.functional.dropout(\n",
    "        attn_weights, p=self.attention_dropout, training=self.training\n",
    "    )\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "        raise ValueError(\n",
    "            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "            f\" {attn_output.size()}\"\n",
    "        )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    if not output_attentions:\n",
    "        attn_weights = None\n",
    "\n",
    "    return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# Replace all default llama attention modules with the new implementation above.\n",
    "def enable_llama_pos_shift_attention(model):\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            enable_llama_pos_shift_attention(\n",
    "                module,\n",
    "            )\n",
    "\n",
    "        if isinstance(module, LlamaAttention):\n",
    "            model._modules[name].forward = types.MethodType(\n",
    "                llama_modified_attention_forward, model._modules[name]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the correctness of the implementation\n",
    "\n",
    "Here we verify the correctness of the attention forward pass implementation.\n",
    "\n",
    "#TODO explain how we are verifying the correctness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_wikitext = load_dataset(\n",
    "        \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\", cache_dir=\"~/repos/datasets\"\n",
    "    )\n",
    "    data_wikitext = \"\\n\\n\".join(\n",
    "        list(filter(lambda x: len(x) > 0, data_wikitext[\"text\"]))\n",
    "    )\n",
    "\n",
    "    return data_wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (288937 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data_wikitext = load_data()\n",
    "\n",
    "encodings = llama3_tokenizer(data_wikitext, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_model_path = \"/Users/aszab/repos/models/Llama-3.2-1B\"\n",
    "llama3_model, llama3_tokenizer = load_model_tokenizer(\n",
    "    llama3_model_path, attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_llama_pos_shift_attention(llama3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_llama3_model, _ = load_model_tokenizer(\n",
    "    llama3_model_path, attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative evaluation on a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my dog is cute and...\n",
      "\n",
      "modified model | reference model\n",
      " playful       | playful\n",
      ",              |,\n",
      " but           | but\n",
      " he            | he\n",
      " is            | is\n",
      " very          | very\n",
      " aggressive    | aggressive\n",
      " and           | and\n",
      " he            | he\n",
      " bites         | bites\n",
      " me            | me\n",
      " when          | when\n",
      " I             | I\n",
      " try           | try\n",
      " to            | to\n",
      " pet           | pet\n",
      " him           | him\n",
      ".              |.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, my dog is cute and\"\n",
    "num_tokens = 17\n",
    "generation = generate_text_kv_caching(\n",
    "    prompt,\n",
    "    llama3_model,\n",
    "    llama3_tokenizer,\n",
    "    num_tokens=num_tokens,\n",
    ")\n",
    "reference_generation = generate_text_kv_caching(\n",
    "    prompt,\n",
    "    reference_llama3_model,\n",
    "    llama3_tokenizer,\n",
    "    num_tokens=num_tokens,\n",
    ")\n",
    "\n",
    "print(prompt + \"...\\n\")\n",
    "print(\"modified model | reference model\")\n",
    "for gen, ref_gen in zip(generation, reference_generation):\n",
    "    print(gen + (len(\"modified model\") - len(gen)) * \" \", ref_gen, sep=\" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test logits given a longer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> = Robert Boulter = \n",
      "\n",
      "\n",
      " Robert Boulter is an English film, television and theatre actor. He had a guest @-@ starring role on the television series The Bill in 2000. This was followed by a starring role in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre. He had a guest role in the television series Judge John Deed in 2002. In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi. He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur, which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London. He was directed by John Tiffany and starred alongside Ben Whishaw, Shane Zaza, Harry Kent, Fraser Ayres, Sophie Stanton and Dominic Hall. \n",
      "\n",
      "\n",
      " In 2006, Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill. He appeared on a 2006 episode of the television series, Doctors, followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke. How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham. Boulter starred in two films in 2008, Daylight Robbery by filmmaker Paris Leonti, and Donkey Punch directed by Olly Blackburn. In May 2008, Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead, followed by an appearance on the television series Survivors in November 2008. He had a recurring role in ten episodes of the television series Casualty in 2010, as \" Kieron Fletcher \". Boulter starred in the 2011 film Mercenaries directed by Paris Leonti. \n",
      "\n",
      "\n",
      " = = Career = = \n",
      "\n",
      "\n",
      " = = = 2000 – 2005 = = = \n",
      "\n",
      "\n",
      " In 2000 Boulter had a guest @-@ starring role on the television series The Bill ; he portrayed \" Scott Parry \" in the episode, \" In Safe Hands \". Boulter starred as \" Scott \" in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre. A review of Boulter's performance in The Independent on Sunday described him\n"
     ]
    }
   ],
   "source": [
    "print(llama3_tokenizer.decode(encodings.input_ids[:, :512][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama3_model(\n",
    "        encodings.input_ids[:, :512].to(device),\n",
    "    )\n",
    "    outputs_final = llama3_model(\n",
    "        encodings.input_ids[:, :512].to(device),\n",
    "        past_key_values=outputs.past_key_values,\n",
    "    )\n",
    "\n",
    "    reference_outputs = reference_llama3_model(\n",
    "        encodings.input_ids[:, :512].to(device),\n",
    "    )\n",
    "    reference_outputs_final = reference_llama3_model(\n",
    "        encodings.input_ids[:, :512].to(device),\n",
    "        past_key_values=reference_outputs.past_key_values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(outputs.logits, reference_outputs.logits)\n",
    "assert torch.equal(outputs_final.logits, reference_outputs_final.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test caching across multiple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = None\n",
    "reference_past_key_values = None\n",
    "\n",
    "for idx in tqdm(range(1024)):\n",
    "    tokens = encodings.input_ids[:, idx : idx + 1].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llama3_model(\n",
    "            tokens,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        reference_outputs = reference_llama3_model(\n",
    "            tokens,\n",
    "            past_key_values=reference_past_key_values,\n",
    "        )\n",
    "        reference_past_key_values = reference_outputs.past_key_values\n",
    "\n",
    "    assert torch.equal(outputs.logits, reference_outputs.logits)\n",
    "\n",
    "    # clean gpu cache on apple silicon\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
